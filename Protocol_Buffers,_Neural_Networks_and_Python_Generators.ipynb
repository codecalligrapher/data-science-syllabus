{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protocol Buffers, Neural Networks and Python Generators.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4w3432R9U8k3fnX9qlB2s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadi350/data-science-syllabus/blob/main/Protocol_Buffers%2C_Neural_Networks_and_Python_Generators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TLDR: So I was working on my thesis, and wanted to implement a particular paper that I would be able to iterate upon, long story short: [this paper](https://ieeexplore.ieee.org/document/8451652) presented a Fully-Convolutional Siamese Neural network for Change Detection. And me, being me, was not satisfied with simply cloning their model [from GitHub](https://github.com/rcdaudt/fully_convolutional_change_detection) and using it as-is. I had to implement it, using TensorFlow (instead of PyTorch), so that I could *really* experience the intricacies of their model. (So I did, and you can find it below in a hidden cell, but that's besides the point of this post)."
      ],
      "metadata": {
        "id": "-mQfU-I2abny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Siamese FCNN Code\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "INPUT_SHAPE = (256, 256, 3)\n",
        "\n",
        "def build_in_channel(name):\n",
        "    inputs = Input(shape=INPUT_SHAPE, name=name)\n",
        "    out_0 = Conv2D(16, (2, 2), padding='same')(inputs)\n",
        "    out_0 = Conv2D(16, (2, 2), padding='same')(out_0)\n",
        "\n",
        "    # shape = 256, 256, 16\n",
        "\n",
        "    out_1 = MaxPooling2D((2, 2))(out_0)\n",
        "    # shape = 128, 128, 16\n",
        "    # 2x2 MaxPool HALVES w and h\n",
        "\n",
        "    out_1 = Conv2D(16, (2, 2), padding='same')(out_1)\n",
        "    out_1 = Conv2D(32, (2, 2), padding='same')(out_1)\n",
        "    out_1 = Conv2D(32, (2, 2), padding='same')(out_1)\n",
        "    # shape = 128, 128, 32\n",
        "\n",
        "    out_2 = MaxPooling2D((2, 2))(out_1)\n",
        "    # shape = 64, 64, 32\n",
        "\n",
        "    out_2 = Conv2D(32, (2, 2), padding='same')(out_2)\n",
        "    out_2 = Conv2D(64, (2, 2), padding='same')(out_2)\n",
        "    out_2 = Conv2D(64, (2, 2), padding='same')(out_2)\n",
        "    out_2 = Conv2D(64, (2, 2), padding='same')(out_2)\n",
        "    # shape = 64, 64, 64\n",
        "\n",
        "    out_3 = MaxPooling2D((2, 2))(out_2)\n",
        "    # shape = 32, 32, 64\n",
        "\n",
        "    out_3 = Conv2D(64, (2, 2),   padding='same')(out_3)\n",
        "    out_3 = Conv2D(128, (2, 2),  padding='same')(out_3)\n",
        "    out_3 = Conv2D(128, (2, 2),  padding='same')(out_3)\n",
        "    out_3 = Conv2D(128, (2, 2),  padding='same')(out_3)\n",
        "    # shape = 32, 32, 128\n",
        "\n",
        "    return inputs, (out_0, out_1, out_2, out_3)\n",
        "\n",
        "\n",
        "def build_siamese_autoencoder():\n",
        "\n",
        "    left_in, (l_out_0, l_out_1, l_out_2, l_out_3) = build_in_channel('left')\n",
        "    right_in, (r_out_0, r_out_1, r_out_2, r_out_3) = build_in_channel('right')\n",
        "\n",
        "    output = subtract([l_out_3, r_out_3])\n",
        "    # shape = 32, 32, 128\n",
        "\n",
        "    l_out_3 = MaxPooling2D((2, 2), padding='same')(l_out_3)\n",
        "    # shape = 16, 16, 128\n",
        "    l_out_3 = Conv2DTranspose(128, 2, padding='same')(l_out_3)\n",
        "    # shape = 16, 16, 128\n",
        "    l_out_3 = Conv2DTranspose(128, 2, padding='same')(l_out_3)\n",
        "    # shape = 16, 16, 128\n",
        "    l_out_3 = UpSampling2D((2, 2))(l_out_3)\n",
        "    # shape = 32, 32, 128\n",
        "\n",
        "    diff_3 = subtract([l_out_3, r_out_3])\n",
        "    output = concatenate([output, diff_3])\n",
        "    # shape = 32, 32, 256\n",
        "\n",
        "    output = Conv2DTranspose(256, 2, padding='same')(output)\n",
        "    # shape = 32, 32, 256\n",
        "    output = Conv2DTranspose(128, 2, padding='same')(output)\n",
        "    # shape = 32, 32, 128\n",
        "    output = Conv2DTranspose(128, 2, padding='same')(output)\n",
        "    # shape = 32, 32, 128\n",
        "    output = Conv2DTranspose(64, 2, padding='same')(output)\n",
        "    # shape = 32, 32, 64\n",
        "    output = Conv2DTranspose(64, 2, padding='same')(output)\n",
        "    # shape = 32, 32, 64\n",
        "    output = Conv2DTranspose(64, 2, padding='same')(output)\n",
        "    # shape = 32, 32, 64\n",
        "    output = UpSampling2D((2, 2))(output)\n",
        "    # shape = 64, 64, 64\n",
        "\n",
        "    diff_2 = subtract([l_out_2, r_out_2])\n",
        "    output = concatenate([output, diff_2])\n",
        "    # shape = 64, 64, 128\n",
        "\n",
        "    output = Conv2DTranspose(128, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(64, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(64, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(32, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(32, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(32, (2, 2), padding='same')(output)\n",
        "    # shape = 64, 64, 32\n",
        "\n",
        "    output = UpSampling2D((2, 2))(output)\n",
        "    # shape = 128, 128, 32\n",
        "\n",
        "    diff_1 = subtract([l_out_1, r_out_1])\n",
        "    output = concatenate([output, diff_1])\n",
        "    # shape = 128, 128, 64\n",
        "\n",
        "    output = Conv2DTranspose(64, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(32, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(16, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(16, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(16, (2, 2), padding='same')(output)\n",
        "    # shape = 128, 128, 16\n",
        "\n",
        "    output = UpSampling2D((2, 2))(output)\n",
        "    # shape = 256, 256, 16\n",
        "\n",
        "    diff_0 = subtract([l_out_0, r_out_0])\n",
        "    output = concatenate([output, diff_0])\n",
        "    # shape = 256, 256, 32\n",
        "    output = Conv2DTranspose(32, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(16, (2, 2), padding='same')(output)\n",
        "    output = Conv2DTranspose(1, (2, 2), padding='same')(output)\n",
        "    # shape =  256, 256, 1\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[left_in, right_in], outputs=output)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AYLU9zXMbYoW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 hours and two days later, I was ready to train my model. A [recent 2022 paper](https://ieeexplore.ieee.org/document/9467555) released a dataset of 20000 image pairs, and painstainkingly labelled masks for the purposes of training the very type of network I had wrote. So there I was, ready with data, my training loop [written from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch) and a freshly brewed cup of coffee, ready to type the all-so-crucial command\n",
        "```bash\n",
        "python src/train.py\n",
        "```\n",
        "\n",
        "But then, after about 15 seconds or so, the stacktrace in my terminal immediately gave me the sense that all was not right....\n",
        "Garbled, nearly unintelligible collections of words, all hinting that I was running out of memory (somehow 64 Gigabytes of system RAM and an 8GB GPU wasn't enough?!), and then, the magic error message brought my model training to a screeching halt indicating something about my \"protos\" did not allow for such large graph-nodes (or something along those lines).\n",
        "\n",
        "A quick side-quest: TensorFlow 2.x default mode of operation is _eager_ mode, when I hit run, the function runs as-is, and does not care on a low-level the command that came before or after. However, if using special decorators, there is a possibility for performance enhancement in using *Graph* execution, where a really smart piece of code optimally choses how to execute my hand-written code in an execution graph. To get a better understanding of this, see the [documentation](https://www.tensorflow.org/guide/intro_to_graphs).\n",
        "\n",
        "# \"proto\"?\n",
        "Now that you have an idea of what Graph-execution is, and a general idea of the error I was facing, there remains one vital gap in information: what the hell is a \"proto\"?! According to [this stackoverflow post](https://stackoverflow.com/questions/34128872/google-protobuf-maximum-size), Protobuf has a hard limit of 2GB, since the arithmetic used is typically 32-bit signed. As [this medium post explained](https://medium.com/@ouwenhuang/tensorflow-graphs-are-just-protobufs-9df51fc7d08d), TF graphs are simply protobufs. Each operation in TensorFlow are symbolic handles for graph-based operations, which are stored as [Protocol Buffers](https://developers.google.com/protocol-buffers). A Protocol Buffer (proto for short), are Google's language-neutral, extensible mechanism for serializing structured data. The specially generated code is used to easily read and write structured data (in this case a TensorFlow graph) regardly of data stream and programming language.\n",
        "\n",
        "To the best of my understanding, my gigantic dataset was causing individual operations in the execution graph to exceed the proto hard-limit of 2GB, since I was using the `tf.Data` API and the `from_tensor_slices` function to keep my entire dataset in memory and perform operations from there. Now, the dataset is about 8GB large, wayyyyy smaller than my 64GB of RAM, however performing multiple layers of convolutions (not to mention, in _parallel_) quickly caused the entire training pipeline to shut down.\n",
        "\n",
        "So I needed to somehow use this large dataset, but without having to keep all the images in memory, and for this, we now move to Python generators\n",
        "\n",
        "# `yield`\n",
        "A *generator* function allows you to declare a function that behaves like an iterator. For example, in order to read lines of a text file, I could do the following, which loads the entire file first, then returns it as a list. The downside of this is that the entire file must be kept in memory\n",
        "\n"
      ],
      "metadata": {
        "id": "JRpclJY8by71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_reader(file_name):\n",
        "    lines = []\n",
        "    for row in open(file_name, \"r\"):\n",
        "        lines.append(row)\n",
        "\n",
        "    return lines"
      ],
      "metadata": {
        "id": "vMgqlehtcng9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If instead, I do the following"
      ],
      "metadata": {
        "id": "62gWUb9rgVy5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PDspQvkZ03X"
      },
      "outputs": [],
      "source": [
        "def csv_reader(file_name):\n",
        "    for row in open(file_name, \"r\"):\n",
        "        yield row"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I could then call the `csv_reader` function as if it were an *iterator*, where the next row is loaded *only when the function is called* and the previous output (possibly already processed) is discarded.\n",
        "\n",
        "# Generators and `tf.Data`\n",
        "\n",
        "TensorFlow's `tf.Data` API is extremely powerful, and the ability to define a Dataset _from a generator_, is all the more powerful. So this is how I solved my issued from above, first I defined a generator for both train and validation sets: \n",
        "\n",
        "(the preprocessing functions section simply loads the image from its file path, converts them to floats and normalizes them)"
      ],
      "metadata": {
        "id": "YpS47IRpgZwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preprocessing Functions\n",
        "def _normalize(img):\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "def decode_grey(img):\n",
        "    img = tf.io.decode_png(img, channels=1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def decode_rgb(img):\n",
        "    img = tf.io.decode_png(img, channels=3)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_path_rgb(fp):\n",
        "    img = tf.io.read_file(fp)\n",
        "    img = decode_rgb(img)\n",
        "    img = _normalize(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def process_path_grey(fp):\n",
        "    img = tf.io.read_file(fp)\n",
        "    img = decode_grey(img)\n",
        "    return img\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1qnk1mjjhHYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gen(split='train', data_path='data/'):\n",
        "    path = data_path + split\n",
        "    for t1, t2, l in zip(sorted(os.listdir(path+'/time1')), sorted(os.listdir(path+'/time2')), sorted(os.listdir(path+'/label'))):\n",
        "        # get full paths\n",
        "\n",
        "        t1 = process_path_rgb(f'data/{split}/time1/' + t1)\n",
        "        t2 = process_path_rgb(f'data/{split}/time2/' + t2)\n",
        "        l = process_path_grey(f'data/{split}/label/' + l)\n",
        "\n",
        "        yield (t1, t2), l\n",
        "\n",
        "def val_gen(split='val', data_path='data/'):\n",
        "    path = data_path + split\n",
        "    for t1, t2, l in zip(sorted(os.listdir(path+'/time1')), sorted(os.listdir(path+'/time2')), sorted(os.listdir(path+'/label'))):\n",
        "        # get full paths\n",
        "\n",
        "        t1 = process_path_rgb(f'data/{split}/time1/' + t1)\n",
        "        t2 = process_path_rgb(f'data/{split}/time2/' + t2)\n",
        "        l = process_path_grey(f'data/{split}/label/' + l)\n",
        "\n",
        "        yield (t1, t2), l"
      ],
      "metadata": {
        "id": "2krhtz1BgZKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not that since my model is a _Siamese_ neural network, it has two *heads* and therefore requires **two** inputs (t1 and t2 above refer to time-1 and time-2, or before-and-after, where l is the label mask indicating the areas that actually underwent change). Finally, I passed these generators to the `tf.Data` API calls as follows:"
      ],
      "metadata": {
        "id": "RSNjy6DRhAoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    train_gen, output_types=((tf.float32, tf.float32), tf.uint8))\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    val_gen, output_types=((tf.float32, tf.float32), tf.uint8))"
      ],
      "metadata": {
        "id": "1fZXvUjvhyy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following section is more for performance and batching, which again removes how much data is actually held in memory at any given point in time. The `from_generator` call achieves exactly what I wanted, where data is loaded on a as-needed basis, and (thus far) avoided my headache with Protocol buffers"
      ],
      "metadata": {
        "id": "WeNurq2_h2_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batching Data\n",
        "buffer_size = 1000\n",
        "batch_size = 200\n",
        "\n",
        "train_batches = (\n",
        "    train_ds\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        "    .repeat()\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "val_batches = (\n",
        "    val_ds\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        "    .repeat()\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hrC3Kl5Mh2OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a very, **very** problem-specific post, however it does cover some key aspects of dealing with large sets of image data, TensorFlow and Python generators. I hope that you learnt something!"
      ],
      "metadata": {
        "id": "0fXUJARKiQnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For any changes, suggestions or overall comments, feel free to reach out to me [on LinkedIn](https://www.linkedin.com/in/aadidev-sooknanan/) or on Twitter [@__aadiDev](https://twitter.com/__aadiDev__)"
      ],
      "metadata": {
        "id": "zaIWCsNhidfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5JUV4KhXi1hc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}